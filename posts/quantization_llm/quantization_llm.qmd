--- 
title: Quantization LLM
author: Samith Va
date: 2024-08-21
categories: [llm, quantization]
format:
    html:
        toc: true 
        code-fold: true
        code-summary: "Show the code"
---

Out of memory error (OOM) is the common problem in fine-tuning LLMs. To tackle with this problem, we use can use q-LoRA to fine tune the model. However, some of the LLMs are not suitable for q-LoRA, only the quantized version (Int4, Int8) is suitable. Another problem is that some of the opensoure LLMs do not provide quantized version.

With Auto-GPTQ we can quantize the LLMs with less resource, since it uses both GPU VRAM, and CPU RAM. This is archievable with consumer devices.

In this post, we will show how to use Auto-GPTQ to quantize an LVLM (Qwen-VL-Guidance). 

First, we need to import all below libraries.

```python
import argparse
import json
from typing import Dict
import logging

import torch
import os 
import transformers
from transformers import AutoTokenizer, AutoModelForCausalLM
from transformers.trainer_pt_utils import LabelSmoother
from auto_gptq import AutoGPTQForCausalLM, BaseQuantizeConfig

```


```python
args = {
    'model_path': 'models/qwen_vl_guidance', # Qwen-VL-Guidance Path 
    'data_path': 'SeeClick/data/mind2web_train_sft.json', # Calibration data path
    'max_len': 2048, # Max sequence length of the model
    'bits': 4, # 4 or 8 for Int4 or Int8
    'group_size': 128, 
    'out_path': '/quantization/qwen-gui-int4' # Output path for quantized model
}
```

`group_size` is the size in layers in a group being quantized, if `group_size` is too large, it will cause large quantization error.

::: {.callout-note}
`data_path` is used for calibration process. It should be large enough, otherwise it will worsen the performance of quantized version model. 
:::

```python
quantize_config = BaseQuantizeConfig(
        bits=args.bits,
        group_size=args.group_size,
        damp_percent=0.01,
        desc_act=False,  # set to False can significantly speed up inference but the perplexity may slightly bad
        static_groups=False,
        sym=True,
        true_sequential=True, 
        model_name_or_path=None,
        model_file_base_name="model"
    )
```

`damp_percent` is set to 1% to avoid quantization error. It is usually set from 1% to 10%. Higher damp percent will increase the smoothing quantization, and lead to more quantization error.

```python
# set up logger
logging.basicConfig(
        format="%(asctime)s %(levelname)s [%(name)s] %(message)s", level=logging.INFO, datefmt="%Y-%m-%d %H:%M:%S"
    )

logging.info("loading tokenizer")
tokenizer = AutoTokenizer.from_pretrained(args.model_path, trust_remote_code=True) # load the tokenizer
tokenizer.pad_token_id = tokenizer.eod_id # set pad_token_id to eod 
logging.info("loading data")
data = preprocess(json.load(open(args.data_path)), tokenizer, args.max_len)

logging.info("loading model")
```

Why do we need to set padding token `tokenizer.pad_token_id` to end of document (EOD) token `tokenizer.eod_id`?

```python
model = AutoGPTQForCausalLM.from_pretrained(args.model_path, 
                                            quantize_config,
                                            low_cpu_mem_usage=True, 
                                            trust_remote_code=True,
                                            torch_dtype=torch.float32 
                                            )
logging.info("model loaded successfully")

model.quantize(data, cache_examples_on_gpu=False) # set cache_examples_on_gpu to False to avoid OOM error


model.save_quantized(args.output_path, use_safetensors=True) # save quantized model
tokenizer.save_pretrained(args.output_path) # save tokenizer for quantized

```

`torch_dtype` should be set to `torch.float32` for CPU running, since `torch.float16` is not supported in CPU.

In summary above, with limited resource we can quantize LLMs into quantized version, and then use q-LoRA to fine tune the model. This quantized model is comparable with the original model, but only required a lot less VRAM on GPUs.



