{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "---\n",
        "title: \"QwenVL Paper Summary\"\n",
        "author: \"Samith Va\"\n",
        "date: \"2024-07-27\"\n",
        "categories: [LLM]\n",
        "format:\n",
        "  html:\n",
        "    code-fold: false\n",
        "    toc: true\n",
        "# jupyter: python3\n",
        "---\n",
        "\n",
        "\n",
        "Qwen-VL is a series of LVLMs which used to understand texts and images, it is developed by Ali group and it is an open-source model.   What’s make it different from Qwen-7B is that with Qwen-VL it introduces **visual receptor**, **language-aligned visual encoder** and **position-aware receptor**.  \n",
        "\n",
        "## Features of Qwen-VL\n",
        "\n",
        "There are 4 features of Qwen-VLs, that make it superior to other LVLMs : \n",
        "\n",
        "1. Leading performance & open source (less model parameters, only 9.6B compare to other LVLMs.)\n",
        "2. Multilingual \n",
        "3. Multiple images input\n",
        "4. More accurate : using higher resolution image in training process (fine-grained visual understanding)\n",
        "\n",
        "## Model Architecture\n",
        "\n",
        "There 3 basic components in Qwen-VL : \n",
        "\n",
        "- Base model : Qwen-7B\n",
        "- Visual Encoder : Vision Transformer (pretrained weights from Openclip’s ViT-bigG)\n",
        "- Position-aware Vision-Language Adaptor : \n",
        "\n",
        "## Training Process\n",
        "\n",
        "The training part consists of 3 processes :\n",
        "\n",
        "1. Pre-train (1.4 billion images, 77.3% in English and 22.7 in Chinese)\n",
        "2. Multi-task Pre-train \n",
        "3. Finetuning (Result in Qwen-VL-Chat)\n",
        "\n",
        "![Pre-train Dataset](pretrain_data.png)\n",
        "\n",
        "\n",
        "\n",
        "### 2. Writing image\n",
        "```python\n",
        "import cv2\n",
        "# Read an image in BGR\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Write the image to a file\n",
        "cv2.imwrite('output_image.jpg', image)\n",
        "```\n",
        "### 3. Coverting color spaces\n",
        "\n",
        "OpenCV provides functions to convert images between different color spaces, such as RGB, BGR, HSV, etc.\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "# Read an image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Convert BGR to grayscale\n",
        "gray_image = cv2.cvtColor(image, cv2.COLOR_BGR2GRAY)\n",
        "\n",
        "# Convert BGR to HSV\n",
        "hsv_image = cv2.cvtColor(image, cv2.COLOR_BGR2HSV)\n",
        "cv2.imwrite('gray_image.jpg', gray_image)\n",
        "cv2.imwrite('hsv_image.jpg', hsv_image)\n",
        "```\n",
        "::: {layout-ncol=2 layout-valign=\"bottom\"}\n",
        "![Grey](gray_image.jpg)\n",
        "\n",
        "![HSV](hsv_image.jpg)\n",
        ":::\n",
        "\n",
        "### 4. Resizing and Cropping Images\n",
        "\n",
        "Resize images to a specific width and height or by a scaling factor. Cropping involves selecting a region of interest (ROI) from the image.\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "# Read an image\n",
        "image = cv2.imread('image.jpg')\n",
        "\n",
        "# Resize the image to a specific width and height\n",
        "new_width, new_height = 200, 200\n",
        "resized_image = cv2.resize(image, (new_width, new_height))\n",
        "\n",
        "# Resize the image by a scaling factor\n",
        "scale_percent = 50  # percent of original size\n",
        "width = int(image.shape[1] * scale_percent / 100)\n",
        "height = int(image.shape[0] * scale_percent / 100)\n",
        "resized_image = cv2.resize(image, (width, height))\n",
        "cv2.imwrite('resized.jpg', resized_image)\n",
        "\n",
        "# Crop a region of interest (ROI) from the image\n",
        "x, y, w, h = 100, 100, 200, 200  # Example coordinates and dimensions\n",
        "cropped_image = image[y:y+h, x:x+w]\n",
        "cv2.imwrite('cropped.jpg', cropped_image)\n",
        "\n",
        "```\n",
        "\n",
        "::: {layout-ncol=2 layout-valign=\"bottom\"}\n",
        "![Resized](resized.jpg)\n",
        "\n",
        "![Cropped](cropped.jpg)\n",
        ":::\n",
        "\n",
        "### 5. Flipping Images\n",
        "\n",
        "The function flip flips the array in one of three different ways (row and column indices are 0-based):\n",
        "`dst = cv.flip( src, flipCode )`\n",
        "`dst`: output array of the same size and type as src.\n",
        "\n",
        "The function has 2 required arguments:\n",
        "\n",
        "- `src`: input image\n",
        "- `flipCode`: a flag to specify how to flip the array; 0 means flipping around the x-axis and positive value (for example, 1) means flipping around y-axis. Negative value (for example, -1) means flipping around both axes.\n"
      ],
      "id": "1150af3f"
    },
    {
      "cell_type": "code",
      "metadata": {},
      "source": [
        "import cv2\n",
        "import matplotlib.pyplot as plt\n",
        "\n",
        "img_bgr = cv2.imread('image.jpg')\n",
        "img_rgb = img_bgr[:, :, ::-1]\n",
        "\n",
        "img_rgb_flipped_horz = cv2.flip(img_rgb, 1)\n",
        "img_rgb_flipped_vert = cv2.flip(img_rgb, 0)\n",
        "img_rgb_flipped_both = cv2.flip(img_rgb, -1)\n",
        "\n",
        "# Show the images\n",
        "plt.figure(figsize=(18, 5))\n",
        "plt.subplot(141) # 141 : 1 row, 4 columns, current index of subplot\n",
        "plt.imshow(img_rgb)\n",
        "plt.title(\"Original\")\n",
        "plt.subplot(142) \n",
        "plt.imshow(img_rgb_flipped_horz)\n",
        "plt.title(\"Horizontal Flip\")\n",
        "plt.subplot(143)\n",
        "plt.imshow(img_rgb_flipped_vert)\n",
        "plt.title(\"Vertical Flip\")\n",
        "plt.subplot(144)\n",
        "plt.imshow(img_rgb_flipped_both)\n",
        "plt.title(\"Both Flipped\")\n",
        "plt.show()"
      ],
      "id": "24f76c8a",
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {},
      "source": [
        "### 6. Reading and Displaying Videos\n",
        "\n",
        "```python\n",
        "import cv2\n",
        "\n",
        "# Open a video file\n",
        "cap = cv2.VideoCapture('video.mp4')\n",
        "\n",
        "while cap.isOpened():\n",
        "    ret, frame = cap.read()\n",
        "    if not ret:\n",
        "        break\n",
        "    cv2.imshow('Video', frame)\n",
        "    if cv2.waitKey(25) & 0xFF == ord('q'):\n",
        "        break\n",
        "\n",
        "cap.release()\n",
        "cv2.destroyAllWindows()\n",
        "\n",
        "```\n",
        "\n",
        "\n",
        "Fore more reference, visit [OpenCV Documentation](https://docs.opencv.org/4.x/index.html)\n"
      ],
      "id": "d19466e5"
    }
  ],
  "metadata": {
    "kernelspec": {
      "name": "python3",
      "language": "python",
      "display_name": "Python 3 (ipykernel)",
      "path": "C:\\Users\\SamithVa\\anaconda3\\envs\\pytorch\\share\\jupyter\\kernels\\python3"
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}